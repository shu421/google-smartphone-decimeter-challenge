{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69d3a760-9217-4c90-9e42-26ba35d6d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(71)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pathlib\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import simdkalman\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import pyproj\n",
    "from pyproj import Proj, transform # 地理的な位置を示す情報を扱うときに、座標系・測地系変換を行ったり、2点間の距離・方位角を計算したりできる。\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b118cb-1466-4108-92eb-e44cde429504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the great circle distance between two points\n",
    "    on the earth. Inputs are array-like and specified in decimal degrees.\n",
    "    \"\"\"\n",
    "    RADIUS = 6_367_000\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    dist = 2 * RADIUS * np.arcsin(a**0.5)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57d0f674-d165-42e9-85e1-f64a852bd1b1",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ground_truth.csv count :  73\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/73 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0c471dc391f46159539737389d1ca85"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# directory setting\n",
    "INPUT = '../input/google-smartphone-decimeter-challenge'\n",
    "\n",
    "base_train = pd.read_csv(INPUT + '/' + 'baseline_locations_train.csv')\n",
    "base_test = pd.read_csv(INPUT + '/' + 'baseline_locations_test.csv')\n",
    "sample_sub = pd.read_csv(INPUT + '/' + 'sample_submission.csv')\n",
    "\n",
    "# ground_truth\n",
    "p = pathlib.Path(INPUT)\n",
    "gt_files = list(p.glob('train/*/*/ground_truth.csv'))\n",
    "print('ground_truth.csv count : ', len(gt_files))\n",
    "\n",
    "gts = []\n",
    "for gt_file in tqdm(gt_files):\n",
    "    gts.append(pd.read_csv(gt_file))\n",
    "ground_truth = pd.concat(gts)\n",
    "ground_truth['phone'] = ground_truth['collectionName'] + '_' + ground_truth['phoneName']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14476743-00af-458f-986d-c8c03e58831d",
   "metadata": {},
   "source": [
    "# Reject outlier\n",
    "- 前と後の距離がそれぞれ50m以上離れていたら削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a0f488a-4938-4044-9103-7e2313fc1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_diff(df):\n",
    "    df['latDeg_prev'] = df['latDeg'].shift(1)\n",
    "    df['latDeg_next'] = df['latDeg'].shift(-1)\n",
    "    df['lngDeg_prev'] = df['lngDeg'].shift(1)\n",
    "    df['lngDeg_next'] = df['lngDeg'].shift(-1)\n",
    "    df['phone_prev'] = df['phone'].shift(1)\n",
    "    df['phone_next'] = df['phone'].shift(-1)\n",
    "    \n",
    "    df['dist_prev'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_prev'], df['lngDeg_prev'])\n",
    "    df['dist_next'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_next'], df['lngDeg_next'])\n",
    "    \n",
    "    df.loc[df['phone']!=df['phone_prev'], ['latDeg_prev', 'lngDeg_prev', 'dist_prev']] = np.nan\n",
    "    df.loc[df['phone']!=df['phone_next'], ['latDeg_next', 'lngDeg_next', 'dist_next']] = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e3ee3-ee3f-431f-a3a8-131b4b017719",
   "metadata": {},
   "source": [
    "# Kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "291d4e9f-1569-4de6-bf2e-bfd412da7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1.0\n",
    "state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\n",
    "observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\n",
    "observation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n",
    "\n",
    "kf = simdkalman.KalmanFilter(\n",
    "        state_transition = state_transition,\n",
    "        process_noise = process_noise,\n",
    "        observation_model = observation_model,\n",
    "        observation_noise = observation_noise)\n",
    "\n",
    "def apply_kf_smoothing(df, kf_=kf):\n",
    "    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n",
    "    for collection, phone in unique_paths:\n",
    "        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n",
    "        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n",
    "        data = data.reshape(1, len(data), 2)\n",
    "        smoothed = kf_.smooth(data)\n",
    "        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0]\n",
    "        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578b25d-bda9-448e-8a63-ffc1038de6d5",
   "metadata": {},
   "source": [
    "# Phone mean prediction\n",
    "- to use the average of the predictions of several phones in the same collection as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97516a1f-57f0-48d8-a259-cbbe6dab4887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lerp_data(df):\n",
    "    '''\n",
    "    Generate interpolated lat,lng values for different phone times in the same collection.\n",
    "    '''\n",
    "    org_columns = df.columns\n",
    "    \n",
    "    # Generate a combination of time x collection x phone and combine it with the original data (generate records to be interpolated)\n",
    "    time_list = df[['collectionName', 'millisSinceGpsEpoch']].drop_duplicates()\n",
    "    phone_list =df[['collectionName', 'phoneName']].drop_duplicates()\n",
    "    tmp = time_list.merge(phone_list, on='collectionName', how='outer')\n",
    "    \n",
    "    lerp_df = tmp.merge(df, on=['collectionName', 'millisSinceGpsEpoch', 'phoneName'], how='left')\n",
    "    lerp_df['phone'] = lerp_df['collectionName'] + '_' + lerp_df['phoneName']\n",
    "    lerp_df = lerp_df.sort_values(['phone', 'millisSinceGpsEpoch'])\n",
    "    \n",
    "    # linear interpolation\n",
    "    lerp_df['latDeg_prev'] = lerp_df['latDeg'].shift(1)\n",
    "    lerp_df['latDeg_next'] = lerp_df['latDeg'].shift(-1)\n",
    "    lerp_df['lngDeg_prev'] = lerp_df['lngDeg'].shift(1)\n",
    "    lerp_df['lngDeg_next'] = lerp_df['lngDeg'].shift(-1)\n",
    "    lerp_df['phone_prev'] = lerp_df['phone'].shift(1)\n",
    "    lerp_df['phone_next'] = lerp_df['phone'].shift(-1)\n",
    "    lerp_df['time_prev'] = lerp_df['millisSinceGpsEpoch'].shift(1)\n",
    "    lerp_df['time_next'] = lerp_df['millisSinceGpsEpoch'].shift(-1)\n",
    "    # Leave only records to be interpolated\n",
    "    lerp_df = lerp_df[(lerp_df['latDeg'].isnull())&(lerp_df['phone']==lerp_df['phone_prev'])&(lerp_df['phone']==lerp_df['phone_next'])].copy()\n",
    "    # calc lerp\n",
    "    lerp_df['latDeg'] = lerp_df['latDeg_prev'] + ((lerp_df['latDeg_next'] - lerp_df['latDeg_prev']) * ((lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev']))) \n",
    "    lerp_df['lngDeg'] = lerp_df['lngDeg_prev'] + ((lerp_df['lngDeg_next'] - lerp_df['lngDeg_prev']) * ((lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev']))) \n",
    "    \n",
    "    # Leave only the data that has a complete set of previous and next data.\n",
    "    lerp_df = lerp_df[~lerp_df['latDeg'].isnull()]\n",
    "    \n",
    "    return lerp_df[org_columns]\n",
    "\n",
    "\n",
    "def calc_mean_pred(df, lerp_df):\n",
    "    '''\n",
    "    Make a prediction based on the average of the predictions of phones in the same collection.\n",
    "    '''\n",
    "    add_lerp = pd.concat([df, lerp_df])\n",
    "    mean_pred_result = add_lerp.groupby(['collectionName', 'millisSinceGpsEpoch'])[['latDeg', 'lngDeg']].mean().reset_index()\n",
    "    mean_pred_df = df[['collectionName', 'phoneName', 'millisSinceGpsEpoch']].copy()\n",
    "    mean_pred_df = mean_pred_df.merge(mean_pred_result[['collectionName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']], on=['collectionName', 'millisSinceGpsEpoch'], how='left')\n",
    "    return mean_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de24cd4e-4610-44e3-b88c-b2fde236ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile50(x):\n",
    "    return np.percentile(x, 50)\n",
    "def percentile95(x):\n",
    "    return np.percentile(x, 95)\n",
    "\n",
    "def get_train_score(df, gt):\n",
    "    gt = gt.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'})\n",
    "    # df = df.merge(gt, on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='inner')\n",
    "    df = df.merge(gt, on=['phone', 'millisSinceGpsEpoch'], how='inner')\n",
    "    # calc_distance_error\n",
    "    df['err'] = calc_haversine(df['latDeg_gt'], df['lngDeg_gt'], df['latDeg'], df['lngDeg'])\n",
    "    # calc_evaluate_score\n",
    "    # df['phone'] = df['collectionName'] + '_' + df['phoneName']\n",
    "    res = df.groupby('phone')['err'].agg([percentile50, percentile95]) # phoneによってgroupbyし、gtと予測値の差(err)の50%,95%値を求める\n",
    "    res['p50_p90_mean'] = (res['percentile50'] + res['percentile95']) / 2 \n",
    "    score = res['p50_p90_mean'].mean()\n",
    "    return score"
   ]
  },
  {
   "source": [
    "# Remove Device "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29519127-2408-4964-aef8-a7f092bf9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(71)\n",
    "\n",
    "def get_removedevice(input_df: pd.DataFrame, device: str) -> pd.DataFrame:\n",
    "    input_df['index'] = input_df.index \n",
    "    input_df = input_df.sort_values('millisSinceGpsEpoch')\n",
    "    input_df.index = input_df['millisSinceGpsEpoch'].values # illisSinceGpsEpochをindexにする\n",
    "\n",
    "    output_df = pd.DataFrame() \n",
    "    for _, subdf in input_df.groupby('collectionName'):\n",
    "\n",
    "        phones = subdf['phoneName'].unique()\n",
    "\n",
    "        # 1つのコレクションにphoneが1種類であるか、対象のデバイスがコレクションに含まれていない時\n",
    "        if (len(phones) == 1) or (not device in phones):\n",
    "            output_df = pd.concat([output_df, subdf])\n",
    "            continue\n",
    "\n",
    "        origin_df = subdf.copy()\n",
    "        \n",
    "        # 対象のデバイスの位置を削除\n",
    "        _index = subdf['phoneName']==device\n",
    "        subdf.loc[_index, 'latDeg'] = np.nan\n",
    "        subdf.loc[_index, 'lngDeg'] = np.nan\n",
    "        \n",
    "        # Nanの周りに値が存在していれば、そのNanを補間\n",
    "        # indexを基準として、線形的に補間していく\n",
    "        subdf = subdf.interpolate(method='index', limit_area='inside')\n",
    "        \n",
    "        # 値が存在しないところは、元の値を使う\n",
    "        _index = subdf['latDeg'].isnull()\n",
    "        subdf.loc[_index, 'latDeg'] = origin_df.loc[_index, 'latDeg'].values\n",
    "        subdf.loc[_index, 'lngDeg'] = origin_df.loc[_index, 'lngDeg'].values\n",
    "\n",
    "        output_df = pd.concat([output_df, subdf])\n",
    "\n",
    "    output_df.index = output_df['index'].values\n",
    "    output_df = output_df.sort_index()\n",
    "\n",
    "    del output_df['index']\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "source": [
    "# Position Shift"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(oof, gt=ground_truth):\n",
    "    df = oof.merge(gt, on = ['phone', 'millisSinceGpsEpoch'])\n",
    "    dst_oof = calc_haversine(df.latDeg_x, df.lngDeg_x, df.latDeg_y, df.lngDeg_y)\n",
    "    scores = pd.DataFrame({'phone': df.phone, 'dst': dst_oof})\n",
    "    scores_grp = scores.groupby('phone') # phoneごとに距離誤差を算出\n",
    "    d50 = scores_grp.quantile(.50).reset_index()\n",
    "    d50.columns = ['phone','q50']\n",
    "    d95 = scores_grp.quantile(.95).reset_index()\n",
    "    d95.columns = ['phone', 'q95']\n",
    "    return (scores_grp.quantile(.50).mean() + scores_grp.quantile(.95).mean())/2, d50.merge(d95)\n",
    "\n",
    "def WGS84_to_ECEF(lat, lon, alt):\n",
    "    # convert to randians\n",
    "    rad_lat = lat * (np.pi / 180.0)\n",
    "    rad_lon = lon * (np.pi / 180.0)\n",
    "    a = 6378137.0 # 地球の長半径\n",
    "    # f is the flattening factor\n",
    "    finv = 298.257223563\n",
    "    f = 1 / finv\n",
    "    e2 = 1 - (1 - f) * (1 - f)\n",
    "    # N is the radius of curvature in the prime vertical\n",
    "    N = a / np.sqrt(1 - e2 * np.sin(rad_lat) * np.sin(rad_lat))\n",
    "    x = (N + alt) * np.cos(rad_lat) * np.cos(rad_lon)\n",
    "    y = (N + alt) * np.cos(rad_lat) * np.sin(rad_lon)\n",
    "    z = (N * (1 - e2) + alt)        * np.sin(rad_lat)\n",
    "    return x, y, z\n",
    "\n",
    "transformer = pyproj.Transformer.from_crs(\n",
    "    {\"proj\":\"geocent\", \"ellps\":\"WGS84\", \"datum\":\"WGS84\"},\n",
    "    {\"proj\":'latlong', \"ellps\":'WGS84', \"datum\":'WGS84'})\n",
    "\n",
    "\n",
    "\n",
    "def ECEF_to_WGS84(x,y,z):\n",
    "    lon, lat, alt = transformer.transform(x,y,z,radians=False)\n",
    "    return lon, lat, alt\n",
    "\n",
    "\n",
    "def position_shift(fname, a):\n",
    "    \n",
    "    d = fname\n",
    "    d['heightAboveWgs84EllipsoidM'] = 63.5\n",
    "    d['x'], d['y'], d['z'] = zip(*d.apply(lambda x: WGS84_to_ECEF(x.latDeg, x.lngDeg, x.heightAboveWgs84EllipsoidM), axis=1))\n",
    "    \n",
    "    # a = -0.2\n",
    "    d.sort_values(['phone', 'millisSinceGpsEpoch'], inplace=True)\n",
    "    for fi in ['x','y','z']:\n",
    "        # 1つ下のphoneが同じところで\n",
    "        d[[fi+'p']] = d[fi].shift(1).where(d['phone'].eq(d['phone'].shift(1)))\n",
    "        # diff: 次の地点との差\n",
    "        d[[fi+'diff']] = d[fi] - d[fi+'p']\n",
    "    # dist: 次の地点との距離\n",
    "    d[['dist']] = np.sqrt(d['xdiff']**2 + d['ydiff']**2 + d['zdiff']**2)\n",
    "    for fi in ['x','y','z']:\n",
    "        d[[fi+'new']] = d[fi+'p'] + d[fi+'diff']*(1-a/d['dist'])\n",
    "    lng, lat, alt = ECEF_to_WGS84(d['xnew'].values, d['ynew'].values, d['znew'].values)\n",
    "    \n",
    "    \n",
    "    lng[np.isnan(lng)] = d.loc[np.isnan(lng),'lngDeg']\n",
    "    lat[np.isnan(lat)]  =d.loc[np.isnan(lat),'latDeg']\n",
    "    d['latDeg'] = lat\n",
    "    d['lngDeg'] = lng\n",
    "    \n",
    "    d.sort_values(['phone','millisSinceGpsEpoch'], inplace=True)\n",
    "\n",
    "    return d[['phone', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']]\n",
    "\n",
    "def objective(trial):\n",
    "    a = trial.suggest_uniform('a', -1, 1)\n",
    "    score, scores = compute_dist(position_shift(filtered, a), ground_truth)\n",
    "    return score"
   ]
  },
  {
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=30)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/shugo/Desktop/Competitions/kaggle/outdoor/venv_outdoor/lib/python3.7/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "ipykernel_launcher:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "kf + reject_outlier :  4.498849117070059\n",
      "+ phones_mean_pred :  4.034688363261449\n",
      "ro, kf, pm, rm, ps :  3.847222643885028\n"
     ]
    }
   ],
   "source": [
    "# reject outlier\n",
    "train_ro = add_distance_diff(base_train)\n",
    "th = 43\n",
    "train_ro.loc[((train_ro['dist_prev'] > th) | (train_ro['dist_next'] > th)), ['latDeg', 'lngDeg']] = np.nan\n",
    "\n",
    "# kalman filter\n",
    "cols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']\n",
    "train_ro_kf = apply_kf_smoothing(train_ro[cols])\n",
    "\n",
    "#phone mean pred\n",
    "train_lerp = make_lerp_data(train_ro_kf)\n",
    "train_mean_pred = calc_mean_pred(train_ro_kf, train_lerp)\n",
    "\n",
    "train_ro_kf['phone'] = train_ro_kf['collectionName'] + '_' + train_ro_kf['phoneName']\n",
    "train_mean_pred['phone'] = train_mean_pred['collectionName'] + '_' + train_mean_pred['phoneName']\n",
    "\n",
    "print('kf + reject_outlier : ', get_train_score(train_ro_kf, ground_truth))\n",
    "print('+ phones_mean_pred : ', get_train_score(train_mean_pred, ground_truth))\n",
    "\n",
    "train_mean_pred = train_mean_pred.drop('collectionName', axis=1)\n",
    "train_mean_pred = train_mean_pred.drop('phoneName', axis=1)\n",
    "train_mean_pred = train_mean_pred.reindex(['phone', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg'], axis='columns')\n",
    "\n",
    "# remove device\n",
    "filtered = train_mean_pred\n",
    "filtered['collectionName'] =filtered['phone'].map(lambda x: x.split('_')[0])\n",
    "filtered['phoneName'] = filtered['phone'].map(lambda x: x.split('_')[1])\n",
    "filtered = get_removedevice(filtered, 'SamsungS20Ultra')\n",
    "\n",
    "filtered = filtered.drop(columns=['collectionName', 'phoneName'], axis=1)\n",
    "\n",
    "filtered = position_shift(filtered, a=0.6602905068929037)\n",
    "filtered.to_csv('../output/filtered_nb025.csv', index=False)\n",
    "\n",
    "# score\n",
    "print('ro, kf, pm, rm, ps : ', get_train_score(filtered, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       millisSinceGpsEpoch         latDeg         lngDeg\n",
       "count         1.313420e+05  131342.000000  131342.000000\n",
       "mean          1.287913e+12      37.435206    -122.154925\n",
       "std           1.162110e+10       0.082713       0.145972\n",
       "min           1.273529e+12      37.322852    -122.428921\n",
       "25%           1.275424e+12      37.371573    -122.275109\n",
       "50%           1.283279e+12      37.424361    -122.118313\n",
       "75%           1.302558e+12      37.469102    -122.069086\n",
       "max           1.303771e+12      37.690806    -121.881870"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>millisSinceGpsEpoch</th>\n      <th>latDeg</th>\n      <th>lngDeg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.313420e+05</td>\n      <td>131342.000000</td>\n      <td>131342.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.287913e+12</td>\n      <td>37.435206</td>\n      <td>-122.154925</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.162110e+10</td>\n      <td>0.082713</td>\n      <td>0.145972</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.273529e+12</td>\n      <td>37.322852</td>\n      <td>-122.428921</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.275424e+12</td>\n      <td>37.371573</td>\n      <td>-122.275109</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.283279e+12</td>\n      <td>37.424361</td>\n      <td>-122.118313</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.302558e+12</td>\n      <td>37.469102</td>\n      <td>-122.069086</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.303771e+12</td>\n      <td>37.690806</td>\n      <td>-121.881870</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "train_ro_kf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c081e-5ca8-4b74-b7ff-9c40f65a45a4",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "25c6415c-622c-478a-a7c4-ab3282e231aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subfileの雛形\n",
    "submission = sample_sub\n",
    "\n",
    "# reject outlier\n",
    "base_test = add_distance_diff(base_test)\n",
    "th = 43\n",
    "base_test.loc[((base_test['dist_prev'] > th) | (base_test['dist_next'] > th)), ['latDeg', 'lngDeg']] = np.nan\n",
    "\n",
    "# kalman filter\n",
    "test_kf = apply_kf_smoothing(base_test)\n",
    "\n",
    "# phone mean pred\n",
    "test_lerp = make_lerp_data(test_kf)\n",
    "test_mean_pred = calc_mean_pred(test_kf, test_lerp)\n",
    "submission['latDeg'] = test_mean_pred['latDeg']\n",
    "submission['lngDeg'] = test_mean_pred['lngDeg']\n",
    "\n",
    "# Remove Device\n",
    "submission['collectionName'] = submission['phone'].map(lambda x: x.split('_')[0])\n",
    "submission['phoneName'] = submission['phone'].map(lambda x: x.split('_')[1])\n",
    "submission = get_removedevice(submission, 'SamsungS20Ultra')\n",
    "submission = submission.drop(columns=['collectionName', 'phoneName'], axis=1)\n",
    "\n",
    "# position shift\n",
    "submission = position_shift(submission, a=0.6602905068929037)\n",
    "\n",
    "submission\n",
    "submission.to_csv('../output/sub_nb025.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('venv_outdoor': venv)",
   "metadata": {
    "interpreter": {
     "hash": "bd00b75c79969edcf008edd1fd5973862c0c93beffacd004fb7d75ad6fcb357f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}