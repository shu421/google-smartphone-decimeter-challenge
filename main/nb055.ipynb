{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(71)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pathlib\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import simdkalman\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from pathlib import Path\n",
    "import pyproj\n",
    "from pyproj import Proj, transform # 地理的な位置を示す情報を扱うときに、座標系・測地系変換を行ったり、2点間の距離・方位角を計算したりできる。\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the great circle distance between two points\n",
    "    on the earth. Inputs are array-like and specified in decimal degrees.\n",
    "    \"\"\"\n",
    "    RADIUS = 6_367_000\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    dist = 2 * RADIUS * np.arcsin(a**0.5)\n",
    "    return dist\n",
    "\n",
    "    \n",
    "def percentile50(x):\n",
    "    return np.percentile(x, 50)\n",
    "def percentile95(x):\n",
    "    return np.percentile(x, 95)\n",
    "\n",
    "def get_train_score(df, gt):\n",
    "    gt = gt.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'})\n",
    "    # df = df.merge(gt, on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='inner')\n",
    "    df = df.merge(gt, on=['phone', 'millisSinceGpsEpoch'], how='inner')\n",
    "    # calc_distance_error\n",
    "    df['err'] = calc_haversine(df['latDeg_gt'], df['lngDeg_gt'], df['latDeg'], df['lngDeg'])\n",
    "    # calc_evaluate_score\n",
    "    # df['phone'] = df['collectionName'] + '_' + df['phoneName']\n",
    "    res = df.groupby('phone')['err'].agg([percentile50, percentile95]) # phoneによってgroupbyし、gtと予測値の差(err)の50%,95%値を求める\n",
    "    res['p50_p90_mean'] = (res['percentile50'] + res['percentile95']) / 2 \n",
    "    score = res['p50_p90_mean'].mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth.csv count :  73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198d5b5a82e941efa01c53358a7c829b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "  0%|          | 0/73 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# directory setting\n",
    "INPUT = '../input/google-smartphone-decimeter-challenge'\n",
    "\n",
    "# base_train = pd.read_csv(INPUT + '/' + 'baseline_locations_train.csv')\n",
    "base_train = pd.read_csv('../output/filtered_nb037.csv')\n",
    "base_train['collectionName'] = base_train['phone'].map(lambda x: x.split('_')[0])\n",
    "base_train['phoneName'] = base_train['phone'].map(lambda x: x.split('_')[1])\n",
    "\n",
    "# base_test = pd.read_csv('../output/sub_nb037.csv')\n",
    "base_test = pd.read_csv('../output/sub_nb037_5.csv')\n",
    "# base_test = pd.read_csv('../output/fixed_base_test.csv')\n",
    "\n",
    "sample_sub = pd.read_csv(INPUT + '/' + 'sample_submission.csv')\n",
    "\n",
    "# ground_truth\n",
    "p = pathlib.Path(INPUT)\n",
    "gt_files = list(p.glob('train/*/*/ground_truth.csv'))\n",
    "print('ground_truth.csv count : ', len(gt_files))\n",
    "\n",
    "gts = []\n",
    "for gt_file in tqdm(gt_files):\n",
    "    gts.append(pd.read_csv(gt_file))\n",
    "ground_truth = pd.concat(gts)\n",
    "ground_truth['phone'] = ground_truth['collectionName'] + '_' + ground_truth['phoneName']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reject outlier\n",
    "- 前と後の距離がそれぞれ50m以上離れていたら削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_diff(df):\n",
    "    df['latDeg_prev'] = df['latDeg'].shift(1)\n",
    "    df['latDeg_next'] = df['latDeg'].shift(-1)\n",
    "    df['lngDeg_prev'] = df['lngDeg'].shift(1)\n",
    "    df['lngDeg_next'] = df['lngDeg'].shift(-1)\n",
    "    df['phone_prev'] = df['phone'].shift(1)\n",
    "    df['phone_next'] = df['phone'].shift(-1)\n",
    "    \n",
    "    df['dist_prev'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_prev'], df['lngDeg_prev'])\n",
    "    df['dist_next'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_next'], df['lngDeg_next'])\n",
    "    \n",
    "    df.loc[df['phone']!=df['phone_prev'], ['latDeg_prev', 'lngDeg_prev', 'dist_prev']] = np.nan\n",
    "    df.loc[df['phone']!=df['phone_next'], ['latDeg_next', 'lngDeg_next', 'dist_next']] = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1.0\n",
    "state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\n",
    "observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\n",
    "observation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n",
    "\n",
    "kf = simdkalman.KalmanFilter(\n",
    "        state_transition = state_transition,\n",
    "        process_noise = process_noise,\n",
    "        observation_model = observation_model,\n",
    "        observation_noise = observation_noise)\n",
    "\n",
    "def apply_kf_smoothing(df_, kf_=kf):\n",
    "    df = df_.copy()\n",
    "    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n",
    "    for collection, phone in unique_paths:\n",
    "        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n",
    "        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n",
    "        data = data.reshape(1, len(data), 2)\n",
    "        smoothed = kf_.smooth(data)\n",
    "        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0]\n",
    "        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phone mean prediction\n",
    "- to use the average of the predictions of several phones in the same collection as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lerp_data(df):\n",
    "    '''\n",
    "    Generate interpolated lat,lng values for different phone times in the same collection.\n",
    "    '''\n",
    "    org_columns = df.columns\n",
    "    \n",
    "    # Generate a combination of time x collection x phone and combine it with the original data (generate records to be interpolated)\n",
    "    time_list = df[['collectionName', 'millisSinceGpsEpoch']].drop_duplicates()\n",
    "    phone_list =df[['collectionName', 'phoneName']].drop_duplicates()\n",
    "    tmp = time_list.merge(phone_list, on='collectionName', how='outer')\n",
    "    \n",
    "    lerp_df = tmp.merge(df, on=['collectionName', 'millisSinceGpsEpoch', 'phoneName'], how='left')\n",
    "    lerp_df['phone'] = lerp_df['collectionName'] + '_' + lerp_df['phoneName']\n",
    "    lerp_df = lerp_df.sort_values(['phone', 'millisSinceGpsEpoch'])\n",
    "    \n",
    "    # linear interpolation\n",
    "    lerp_df['latDeg_prev'] = lerp_df['latDeg'].shift(1)\n",
    "    lerp_df['latDeg_next'] = lerp_df['latDeg'].shift(-1)\n",
    "    lerp_df['lngDeg_prev'] = lerp_df['lngDeg'].shift(1)\n",
    "    lerp_df['lngDeg_next'] = lerp_df['lngDeg'].shift(-1)\n",
    "    lerp_df['phone_prev'] = lerp_df['phone'].shift(1)\n",
    "    lerp_df['phone_next'] = lerp_df['phone'].shift(-1)\n",
    "    lerp_df['time_prev'] = lerp_df['millisSinceGpsEpoch'].shift(1)\n",
    "    lerp_df['time_next'] = lerp_df['millisSinceGpsEpoch'].shift(-1)\n",
    "    # Leave only records to be interpolated\n",
    "    lerp_df = lerp_df[(lerp_df['latDeg'].isnull())&(lerp_df['phone']==lerp_df['phone_prev'])&(lerp_df['phone']==lerp_df['phone_next'])].copy()\n",
    "    # calc lerp\n",
    "    lerp_df['latDeg'] = lerp_df['latDeg_prev'] + ((lerp_df['latDeg_next'] - lerp_df['latDeg_prev']) * ((lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev']))) \n",
    "    lerp_df['lngDeg'] = lerp_df['lngDeg_prev'] + ((lerp_df['lngDeg_next'] - lerp_df['lngDeg_prev']) * ((lerp_df['millisSinceGpsEpoch'] - lerp_df['time_prev']) / (lerp_df['time_next'] - lerp_df['time_prev']))) \n",
    "    \n",
    "    # Leave only the data that has a complete set of previous and next data.\n",
    "    lerp_df = lerp_df[~lerp_df['latDeg'].isnull()]\n",
    "    \n",
    "    return lerp_df[org_columns]\n",
    "\n",
    "\n",
    "def calc_mean_pred(df, lerp_df):\n",
    "    '''\n",
    "    Make a prediction based on the average of the predictions of phones in the same collection.\n",
    "    '''\n",
    "    add_lerp = pd.concat([df, lerp_df])\n",
    "    mean_pred_result = add_lerp.groupby(['collectionName', 'millisSinceGpsEpoch'])[['latDeg', 'lngDeg']].mean().reset_index()\n",
    "    mean_pred_df = df[['collectionName', 'phoneName', 'millisSinceGpsEpoch']].copy()\n",
    "    mean_pred_df = mean_pred_df.merge(mean_pred_result[['collectionName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']], on=['collectionName', 'millisSinceGpsEpoch'], how='left')\n",
    "    return mean_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(71)\n",
    "\n",
    "def get_removedevice(input_df: pd.DataFrame, device: str) -> pd.DataFrame:\n",
    "    input_df['index'] = input_df.index \n",
    "    input_df = input_df.sort_values('millisSinceGpsEpoch')\n",
    "    input_df.index = input_df['millisSinceGpsEpoch'].values # illisSinceGpsEpochをindexにする\n",
    "\n",
    "    output_df = pd.DataFrame() \n",
    "    for _, subdf in input_df.groupby('collectionName'):\n",
    "\n",
    "        phones = subdf['phoneName'].unique()\n",
    "\n",
    "        # 1つのコレクションにphoneが1種類であるか、対象のデバイスがコレクションに含まれていない時\n",
    "        if (len(phones) == 1) or (not device in phones):\n",
    "            output_df = pd.concat([output_df, subdf])\n",
    "            continue\n",
    "\n",
    "        origin_df = subdf.copy()\n",
    "        \n",
    "        # 対象のデバイスの位置を削除\n",
    "        _index = subdf['phoneName']==device\n",
    "        subdf.loc[_index, 'latDeg'] = np.nan\n",
    "        subdf.loc[_index, 'lngDeg'] = np.nan\n",
    "        \n",
    "        # Nanの周りに値が存在していれば、そのNanを補間\n",
    "        # indexを基準として、線形的に補間していく\n",
    "        subdf = subdf.interpolate(method='index', limit_area='inside')\n",
    "        \n",
    "        # 値が存在しないところは、元の値を使う\n",
    "        _index = subdf['latDeg'].isnull()\n",
    "        subdf.loc[_index, 'latDeg'] = origin_df.loc[_index, 'latDeg'].values\n",
    "        subdf.loc[_index, 'lngDeg'] = origin_df.loc[_index, 'lngDeg'].values\n",
    "\n",
    "        output_df = pd.concat([output_df, subdf])\n",
    "\n",
    "    output_df.index = output_df['index'].values\n",
    "    output_df = output_df.sort_index()\n",
    "\n",
    "    del output_df['index']\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(oof, gt=ground_truth):\n",
    "    df = oof.merge(gt, on = ['phone', 'millisSinceGpsEpoch'])\n",
    "    dst_oof = calc_haversine(df.latDeg_x, df.lngDeg_x, df.latDeg_y, df.lngDeg_y)\n",
    "    scores = pd.DataFrame({'phone': df.phone, 'dst': dst_oof})\n",
    "    scores_grp = scores.groupby('phone') # phoneごとに距離誤差を算出\n",
    "    d50 = scores_grp.quantile(.50).reset_index()\n",
    "    d50.columns = ['phone','q50']\n",
    "    d95 = scores_grp.quantile(.95).reset_index()\n",
    "    d95.columns = ['phone', 'q95']\n",
    "    return (scores_grp.quantile(.50).mean() + scores_grp.quantile(.95).mean())/2, d50.merge(d95)\n",
    "\n",
    "def WGS84_to_ECEF(lat, lon, alt):\n",
    "    # convert to randians\n",
    "    rad_lat = lat * (np.pi / 180.0)\n",
    "    rad_lon = lon * (np.pi / 180.0)\n",
    "    a = 6378137.0 # 地球の長半径\n",
    "    # f is the flattening factor\n",
    "    finv = 298.257223563\n",
    "    f = 1 / finv\n",
    "    e2 = 1 - (1 - f) * (1 - f)\n",
    "    # N is the radius of curvature in the prime vertical\n",
    "    N = a / np.sqrt(1 - e2 * np.sin(rad_lat) * np.sin(rad_lat))\n",
    "    x = (N + alt) * np.cos(rad_lat) * np.cos(rad_lon)\n",
    "    y = (N + alt) * np.cos(rad_lat) * np.sin(rad_lon)\n",
    "    z = (N * (1 - e2) + alt)        * np.sin(rad_lat)\n",
    "    return x, y, z\n",
    "\n",
    "transformer = pyproj.Transformer.from_crs(\n",
    "    {\"proj\":\"geocent\", \"ellps\":\"WGS84\", \"datum\":\"WGS84\"},\n",
    "    {\"proj\":'latlong', \"ellps\":'WGS84', \"datum\":'WGS84'})\n",
    "\n",
    "\n",
    "\n",
    "def ECEF_to_WGS84(x,y,z):\n",
    "    lon, lat, alt = transformer.transform(x,y,z,radians=False)\n",
    "    return lon, lat, alt\n",
    "\n",
    "\n",
    "def position_shift(fname, a):\n",
    "    \n",
    "    d = fname\n",
    "    d['heightAboveWgs84EllipsoidM'] = 63.5\n",
    "    d['x'], d['y'], d['z'] = zip(*d.apply(lambda x: WGS84_to_ECEF(x.latDeg, x.lngDeg, x.heightAboveWgs84EllipsoidM), axis=1))\n",
    "    \n",
    "    # a = -0.2\n",
    "    d.sort_values(['phone', 'millisSinceGpsEpoch'], inplace=True)\n",
    "    for fi in ['x','y','z']:\n",
    "        # 1つ下のphoneが同じところで\n",
    "        d[[fi+'p']] = d[fi].shift(1).where(d['phone'].eq(d['phone'].shift(1)))\n",
    "        # diff: 次の地点との差\n",
    "        d[[fi+'diff']] = d[fi] - d[fi+'p']\n",
    "    # dist: 次の地点との距離\n",
    "    d[['dist']] = np.sqrt(d['xdiff']**2 + d['ydiff']**2 + d['zdiff']**2)\n",
    "    for fi in ['x','y','z']:\n",
    "        d[[fi+'new']] = d[fi+'p'] + d[fi+'diff']*(1-a/d['dist'])\n",
    "    lng, lat, alt = ECEF_to_WGS84(d['xnew'].values, d['ynew'].values, d['znew'].values)\n",
    "    \n",
    "    \n",
    "    lng[np.isnan(lng)] = d.loc[np.isnan(lng),'lngDeg']\n",
    "    lat[np.isnan(lat)] = d.loc[np.isnan(lat),'latDeg']\n",
    "    d['latDeg'] = lat\n",
    "    d['lngDeg'] = lng\n",
    "    \n",
    "    d.sort_values(['phone','millisSinceGpsEpoch'], inplace=True)\n",
    "\n",
    "    return d[['phone', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']]\n",
    "\n",
    "def objective(trial):\n",
    "    a = trial.suggest_uniform('a', -1, 1)\n",
    "    score, scores = compute_dist(position_shift(filtered, a), ground_truth)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove low Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_nogt_diff(df):\n",
    "    # shift(1): 上のやつが1個下に下がる → 前のデータ\n",
    "    # shift(-1): 下のやつが1個上に上がる → 次のデータ\n",
    "    df['latDeg_prev'] = df['latDeg'].shift(1)\n",
    "    df['latDeg_next'] = df['latDeg'].shift(-1)\n",
    "    df['lngDeg_prev'] = df['lngDeg'].shift(1)\n",
    "    df['lngDeg_next'] = df['lngDeg'].shift(-1)\n",
    "    df['phone_prev'] = df['phone'].shift(1)\n",
    "    df['phone_next'] = df['phone'].shift(-1)\n",
    "    \n",
    "    df['latDeg_prev_diff'] = df['latDeg'] - df['latDeg_prev']\n",
    "    df['latDeg_next_diff'] = df['latDeg_next'] - df['latDeg']\n",
    "    \n",
    "    df['lngDeg_prev_diff'] = df['lngDeg'] - df['lngDeg_prev']\n",
    "    df['lngDeg_next_diff'] = df['lngDeg_next'] - df['lngDeg']\n",
    "\n",
    "    \n",
    "    df['dist_prev'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_prev'], df['lngDeg_prev'])\n",
    "    df['dist_next'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_next'], df['lngDeg_next'])\n",
    "    \n",
    "    \n",
    "    df.loc[df['phone']!=df['phone_prev'], ['latDeg_prev', 'lngDeg_prev', 'dist_prev', \n",
    "                                          'latDeg_prev_diff', 'lngDeg_prev_diff']] = np.nan\n",
    "    \n",
    "    df.loc[df['phone']!=df['phone_next'], ['latDeg_next', 'lngDeg_next', 'dist_next', \n",
    "                                           'latDeg_next_diff', 'lngDeg_next_diff']] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_lowSpeed(_df, dist_thr=0.4):\n",
    "    df = _df.copy()\n",
    "    df['latDeg'] = df['latDeg'].astype(float)\n",
    "    df['lngDeg'] = df['lngDeg'].astype(float)\n",
    "\n",
    "    df = add_distance_nogt_diff(df)\n",
    "\n",
    "    _index = df[(df['dist_prev']<dist_thr) | (df['dist_next']<dist_thr)]['latDeg'].index\n",
    "    df.loc[_index, 'latDeg'] = np.nan\n",
    "    df.loc[_index, 'lngDeg'] = np.nan\n",
    "    # phoneごとに補間する\n",
    "    dfs = []\n",
    "    for _, df in df.groupby('phone'):\n",
    "        df = df.interpolate(method='linear',\n",
    "                            limit=None,\n",
    "                            limit_direction='both')\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    return df[['phone','millisSinceGpsEpoch','latDeg','lngDeg']]\n",
    "\n",
    "\n",
    "def objective_rmls(trial):\n",
    "    x = trial.suggest_uniform('x', 0.5, 0.9)\n",
    "    score = get_train_score(remove_lowSpeed(filtered, x), ground_truth)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective_rmls, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phones mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_with_other_phones(df_):\n",
    "    df = df_.copy()\n",
    "\n",
    "    collections_list = df[['collectionName']].drop_duplicates().to_numpy()\n",
    "\n",
    "    for collection in collections_list:\n",
    "        phone_list = df[df['collectionName'].to_list() == collection][['phoneName']].drop_duplicates().to_numpy()\n",
    "\n",
    "        phone_data = {}\n",
    "        corrections = {}\n",
    "        for phone in phone_list:\n",
    "            cond = np.logical_and(df['collectionName'] == collection[0], df['phoneName'] == phone[0]).to_list()\n",
    "            phone_data[phone[0]] = df[cond][['millisSinceGpsEpoch', 'latDeg', 'lngDeg']].to_numpy()\n",
    "\n",
    "        for current in phone_data:\n",
    "            correction = np.ones(phone_data[current].shape, dtype=np.float64)\n",
    "            correction[:,1:] = phone_data[current][:,1:]\n",
    "            \n",
    "            # Telephones data don't complitely match by time, so - interpolate.\n",
    "            for other in phone_data:\n",
    "                if other == current:\n",
    "                    continue\n",
    "\n",
    "                loc = interp1d(phone_data[other][:,0], \n",
    "                               phone_data[other][:,1:], \n",
    "                               axis=0, \n",
    "                               kind='linear', \n",
    "                               copy=False, \n",
    "                               bounds_error=None, \n",
    "                               fill_value='extrapolate', \n",
    "                               assume_sorted=True)\n",
    "                \n",
    "                start_idx = 0\n",
    "                stop_idx = 0\n",
    "                for idx, val in enumerate(phone_data[current][:,0]):\n",
    "                    if val < phone_data[other][0,0]:\n",
    "                        start_idx = idx\n",
    "                    if val < phone_data[other][-1,0]:\n",
    "                        stop_idx = idx\n",
    "\n",
    "                if stop_idx - start_idx > 0:\n",
    "                    correction[start_idx:stop_idx,0] += 1\n",
    "                    correction[start_idx:stop_idx,1:] += loc(phone_data[current][start_idx:stop_idx,0])                    \n",
    "\n",
    "            correction[:,1] /= correction[:,0]\n",
    "            correction[:,2] /= correction[:,0]\n",
    "            \n",
    "            corrections[current] = correction.copy()\n",
    "        \n",
    "        for phone in phone_list:\n",
    "            cond = np.logical_and(df['collectionName'] == collection[0], df['phoneName'] == phone[0]).to_list()\n",
    "            \n",
    "            df.loc[cond, ['latDeg', 'lngDeg']] = corrections[phone[0]][:,1:]            \n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snap to grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_line_points(df, num_interpolate=5):\n",
    "    # create grid point\n",
    "    line_points = df[['latDeg','lngDeg']].copy()\n",
    "    switches = line_points.ne(line_points.shift(-1))\n",
    "    idx = switches[switches].index\n",
    "    num_interpolate = num_iterpolate\n",
    "    for i in range(num_interpolate):\n",
    "        df_new = pd.DataFrame(index=idx + 0.5)\n",
    "        line_points= pd.concat([line_points, df_new]).sort_index()\n",
    "    line_points = line_points.reset_index(drop=True)\n",
    "    line_points = line_points.interpolate(method='linear')\n",
    "    return line_points\n",
    "\n",
    "\n",
    "def snap_to_grid(input_df, target_collection=None, line_points, max_thr=50, min_thr=0):\n",
    "\n",
    "    if target_collection:\n",
    "        input_df =input_df[input_df['collectionName']==target_collection]\n",
    "\n",
    "    def find_closest_point(point, points, max_thr=19, min_thr=16):\n",
    "        \"\"\" Find closest point from a list of points. \"\"\"\n",
    "        df_ = pd.DataFrame({'latDeg':point['latDeg'].repeat(len(points)), \n",
    "                            'lngDeg':point['lngDeg'].repeat(len(points))},\n",
    "                            columns=['latDeg', 'lngDeg'])\n",
    "        # return minimum distance points\n",
    "        distance = calc_haversine(points['latDeg'], points['lngDeg'],\n",
    "                                    df_['latDeg'], df_['lngDeg']).min()\n",
    "        if min_thr <= distance <= max_thr:\n",
    "            return list(points.loc[calc_haversine(points['latDeg'], points['lngDeg'],\n",
    "                                df_['latDeg'], df_['lngDeg']).argmin()])\n",
    "\n",
    "    def apply_grid_point(x, closest_point):\n",
    "        '''\n",
    "        input: \n",
    "            x: train row\n",
    "            closest_point: closest point or None\n",
    "        '''\n",
    "        idx = x.name\n",
    "        closest_point1 = closest_point[closest_point.index==idx]\n",
    "        if closest_point1.isnull().values == True:\n",
    "            pass\n",
    "        else:\n",
    "            x['latDeg'] = closest_point1.values[0][0]\n",
    "            x['lngDeg'] = closest_point1.values[0][1]\n",
    "        return x\n",
    "\n",
    "    df = input_df.copy()\n",
    "    closest_point = df[['latDeg','lngDeg']].parallel_apply(lambda x: find_closest_point(x, line_points[['latDeg', 'lngDeg']], max_thr=max_thr, min_thr=min_thr), axis=1)\n",
    "    df[['latDeg', 'lngDeg']] = df[['latDeg', 'lngDeg']].parallel_apply(apply_grid_point, closest_point=closest_point, axis=1)\n",
    "\n",
    "    input_df.\n",
    "    return output_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reject outlier + kalmanfilter:  4.498849117070059\n",
      "phone mean pred :  4.034688363261449\n",
      "phones mean : 3.5362898482112963\n",
      "remove low speed:  3.486164724134517\n",
      "position shift:  3.4184543506885996\n",
      "ro, kf, pm, rm, psm, rmls, ps:  3.4184543506885996\n"
     ]
    }
   ],
   "source": [
    "# reject outlier\n",
    "train_ro = add_distance_diff(base_train)\n",
    "th = 43\n",
    "train_ro.loc[((train_ro['dist_prev'] > th) | (train_ro['dist_next'] > th)), ['latDeg', 'lngDeg']] = np.nan\n",
    "\n",
    "# kalman filter\n",
    "cols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']\n",
    "train_ro_kf = apply_kf_smoothing(train_ro[cols])\n",
    "\n",
    "# phone mean pred\n",
    "train_lerp = make_lerp_data(train_ro_kf)\n",
    "train_mean_pred = calc_mean_pred(train_ro_kf, train_lerp)\n",
    "\n",
    "train_ro_kf['phone'] = train_ro_kf['collectionName'] + '_' + train_ro_kf['phoneName']\n",
    "train_mean_pred['phone'] = train_mean_pred['collectionName'] + '_' + train_mean_pred['phoneName']\n",
    "\n",
    "print('reject outlier + kalmanfilter: ', get_train_score(train_ro_kf, ground_truth))\n",
    "print('phone mean pred : ', get_train_score(train_mean_pred, ground_truth))\n",
    "\n",
    "train_mean_pred = train_mean_pred.drop('collectionName', axis=1)\n",
    "train_mean_pred = train_mean_pred.drop('phoneName', axis=1)\n",
    "train_mean_pred = train_mean_pred.reindex(['phone', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg'], axis='columns')\n",
    "filtered = train_mean_pred\n",
    "\n",
    "# remove device\n",
    "filtered['collectionName'] =filtered['phone'].map(lambda x: x.split('_')[0])\n",
    "filtered['phoneName'] = filtered['phone'].map(lambda x: x.split('_')[1])\n",
    "filtered = get_removedevice(filtered, 'SamsungS20Ultra')\n",
    "filtered = filtered.drop(columns=['collectionName', 'phoneName'], axis=1)\n",
    "\n",
    "# phones mean\n",
    "filtered['collectionName'] =filtered['phone'].map(lambda x: x.split('_')[0])\n",
    "filtered['phoneName'] = filtered['phone'].map(lambda x: x.split('_')[1])\n",
    "filtered = mean_with_other_phones(filtered)\n",
    "filtered = filtered.drop(columns=['collectionName', 'phoneName'], axis=1)\n",
    "print('phones mean :', get_train_score(filtered, ground_truth))\n",
    "\n",
    "# remove lowSpeed\n",
    "filtered = remove_lowSpeed(filtered, 0.6939300630849313)\n",
    "print('remove low speed: ', get_train_score(filtered, ground_truth))\n",
    "\n",
    "# position shift\n",
    "filtered = position_shift(filtered, a=0.6602905068929037)\n",
    "print('position shift: ', get_train_score(filtered, ground_truth))\n",
    "\n",
    "# to csv\n",
    "filtered.to_csv('../output/filtered_nb046.csv', index=False)\n",
    "\n",
    "# score\n",
    "print('ro, kf, pm, rm, psm, rmls, ps: ', get_train_score(filtered, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subfileの雛形\n",
    "submission = sample_sub\n",
    "\n",
    "# reject outlier\n",
    "base_test = add_distance_diff(base_test)\n",
    "th = 43\n",
    "base_test.loc[((base_test['dist_prev'] > th) | (base_test['dist_next'] > th)), ['latDeg', 'lngDeg']] = np.nan\n",
    "\n",
    "# kalman filter\n",
    "test_kf = apply_kf_smoothing(base_test)\n",
    "\n",
    "# phone mean pred\n",
    "test_lerp = make_lerp_data(test_kf)\n",
    "test_mean_pred = calc_mean_pred(test_kf, test_lerp)\n",
    "submission['latDeg'] = test_mean_pred['latDeg']\n",
    "submission['lngDeg'] = test_mean_pred['lngDeg']\n",
    "\n",
    "# Remove Device\n",
    "submission['collectionName'] = submission['phone'].map(lambda x: x.split('_')[0])\n",
    "submission['phoneName'] = submission['phone'].map(lambda x: x.split('_')[1])\n",
    "submission = get_removedevice(submission, 'SamsungS20Ultra')\n",
    "submission = submission.drop(columns=['collectionName', 'phoneName'], axis=1)\n",
    "\n",
    "# phones mean\n",
    "submission['collectionName'] =submission['phone'].map(lambda x: x.split('_')[0])\n",
    "submission['phoneName'] = submission['phone'].map(lambda x: x.split('_')[1])\n",
    "submission = mean_with_other_phones(submission)\n",
    "submission = submission.drop(columns=['collectionName', 'phoneName'], axis=1)\n",
    "\n",
    "# remove lowSpeed\n",
    "submission = remove_lowSpeed(submission, 0.6939300630849313)\n",
    "\n",
    "# position shift\n",
    "submission = position_shift(submission, a=0.6602905068929037)\n",
    "\n",
    "# submission\n",
    "# submission.to_csv('../output/sub_nb046.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4  ('venv_outdoor': venv)",
   "metadata": {
    "interpreter": {
     "hash": "bd00b75c79969edcf008edd1fd5973862c0c93beffacd004fb7d75ad6fcb357f"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}