{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "# from cv2 import Rodrigues\n",
    "from math import sin, cos, atan2, sqrt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyproj\n",
    "from pyproj import Proj, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pathlib\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../input/google-smartphone-decimeter-challenge\")\n",
    "bl_trn_fname = 'baseline_locations_train.csv'\n",
    "bl_tst_fname = 'baseline_locations_test.csv'\n",
    "sample_fname = 'sample_submission.csv'\n",
    "\n",
    "# ground truth\n",
    "p = pathlib.Path(data_dir)\n",
    "gt_files = list(p.glob('train/*/*/ground_truth.csv'))\n",
    "gts = []\n",
    "for gt_file in gt_files:\n",
    "    gts.append(pd.read_csv(gt_file))\n",
    "ground_truth = pd.concat(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_trn_df = pd.read_csv(data_dir / bl_trn_fname)\n",
    "bl_tst_df = pd.read_csv(data_dir / bl_tst_fname)\n",
    "sample_df = pd.read_csv(data_dir / sample_fname)\n",
    "\n",
    "# filtered_nb025 = pd.read_csv('../output/filtered_nb025.csv')\n",
    "# bl_trn_df['latDeg'] = filtered_nb025['latDeg']\n",
    "# bl_trn_df['lngDeg'] = filtered_nb025['lngDeg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Euler Angles to Rotation Vector  \n",
    "Euler Angles <-> Rotation Matrix <-> Rotation Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pitch:y\n",
    "# yaw:z\n",
    "# roll:x\n",
    "def an2v(y_delta, z_delta, x_delta):\n",
    "    '''\n",
    "    Euler Angles ->Rotation Matrix -> Rotation Vector\n",
    "\n",
    "    Input：\n",
    "        1. y_delta          (float): the angle with rotateing around y-axis.\n",
    "        2. z_delta         (float): the angle with rotateing around z-axis. \n",
    "        3. x_delta         (float): the angle with rotateing around x-axis. \n",
    "    Output：\n",
    "        rx/ry/rz             (float): the rotation vector with rotateing \n",
    "    \n",
    "    Code Ref.: https://www.zacobria.com/universal-robots-knowledge-base-tech-support-forum-hints-tips/python-code-example-of-converting-rpyeuler-angles-to-rotation-vectorangle-axis-for-universal-robots/\n",
    "    (Note：In Code Ref: pitch=y,yaw=z,roll=x. But Google is pitch=x,yaw=z,roll=y)\n",
    "    '''\n",
    "    # yaw: z\n",
    "    Rz_Matrix = np.matrix([\n",
    "    [math.cos(z_delta), -math.sin(z_delta), 0],\n",
    "    [math.sin(z_delta), math.cos(z_delta), 0],\n",
    "    [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # pitch: y\n",
    "    Ry_Matrix = np.matrix([\n",
    "    [math.cos(y_delta), 0, math.sin(y_delta)],\n",
    "    [0, 1, 0],\n",
    "    [-math.sin(y_delta), 0, math.cos(y_delta)]\n",
    "    ])\n",
    "    \n",
    "    # roll: x\n",
    "    Rx_Matrix = np.matrix([\n",
    "    [1, 0, 0],\n",
    "    [0, math.cos(x_delta), -math.sin(x_delta)],\n",
    "    [0, math.sin(x_delta), math.cos(x_delta)]\n",
    "    ])\n",
    "\n",
    "    R = Rz_Matrix * Ry_Matrix * Rx_Matrix\n",
    "\n",
    "    theta = math.acos(((R[0, 0] + R[1, 1] + R[2, 2]) - 1) / 2)\n",
    "    multi = 1 / (2 * math.sin(theta))\n",
    "\n",
    "    rx = multi * (R[2, 1] - R[1, 2]) * theta\n",
    "    ry = multi * (R[0, 2] - R[2, 0]) * theta\n",
    "    rz = multi * (R[1, 0] - R[0, 1]) * theta\n",
    "\n",
    "    return rx, ry, rz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2a(rotation_v):\n",
    "    '''\n",
    "    Rotation Vector -> Rotation Matrix -> Euler Angles\n",
    "\n",
    "    Input：\n",
    "        rx/ry/rz             (float): the rotation vector with rotateing around x/y/z-axis.\n",
    "    Output：\n",
    "        1. y_delta          (float): the angle with rotateing around y-axis.\n",
    "        2. z_delta         (float): the angle with rotateing around z-axis. \n",
    "        3. x_delta         (float): the angle with rotateing around x-axis.  \n",
    "    '''\n",
    "    # Rotation Vector -> Rotation Matrix\n",
    "    R = Rodrigues(rotation_v)[0]\n",
    "\n",
    "    sq = sqrt(R[2,1] ** 2 +  R[2,2] ** 2)\n",
    "\n",
    "    if  not (sq < 1e-6) :\n",
    "        x_delta = atan2(R[2,1] , R[2,2])\n",
    "        y_delta = atan2(-R[2,0], sq)\n",
    "        z_delta = atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x_delta = atan2(-R[1,2], R[1,1])\n",
    "        y_delta = atan2(-R[2,0], sq)\n",
    "        z_delta = 0\n",
    "\n",
    "    return y_delta, z_delta, x_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare IMU Dataset  \n",
    "This part is to prepare the dataset for the model. I divided this part into the following steps:  \n",
    "(1) Load GNSS Log  \n",
    "(2) Merge sub-dataset (Status/UncalAccel/UncalGyro/UncalMag/OrientationDeg)  \n",
    "(3) UTC to GpsEpoch  \n",
    "(4) OrientationDeg to Rotation Vector  \n",
    "(5) Calibrate Sensors' data  \n",
    "(6) LatDeg&lngDeg to x/y/z  \n",
    "(7) Orgainze Data (eg. t1 t2 t3 t4 t5 -> t6)  \n",
    "(8) Clean Data (unrelated-aixs features and uncalibrated features)  \n",
    "(9) Add Statistic Features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnss_log_to_dataframes(path):\n",
    "    '''Load GNSS Log'''\n",
    "    print('Loading ' + path, flush = True)\n",
    "    gnss_section_names = {'Raw', 'UncalAccel', 'UncalGyro', 'UncalMag', 'Fix', 'Status', 'OrientationDeg'}\n",
    "    with open(path) as f_open:\n",
    "        datalines = f_open.readlines()\n",
    "\n",
    "    datas = {k: [] for k in gnss_section_names}\n",
    "    gnss_map = {k: [] for k in gnss_section_names}\n",
    "    for dataline in datalines:\n",
    "        is_header = dataline.startswith('#')\n",
    "        dataline = dataline.strip('#').strip().split(',')\n",
    "        # skip over notes, version numbers, etc\n",
    "        if is_header and dataline[0] in gnss_section_names:\n",
    "            gnss_map[dataline[0]] = dataline[1:]\n",
    "        elif not is_header:\n",
    "            datas[dataline[0]].append(dataline[1:])\n",
    "\n",
    "    results = dict()\n",
    "    for k, v in datas.items():\n",
    "        results[k] = pd.DataFrame(v, columns=gnss_map[k])\n",
    "    # pandas doesn't properly infer types from these lists by default\n",
    "    for k, df in results.items():\n",
    "        for col in df.columns:\n",
    "            if col == 'CodeType':\n",
    "                continue\n",
    "            results[k][col] = pd.to_numeric(results[k][col])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UTC2GpsEpoch(df):\n",
    "    '''UTC to GpsEpoch\n",
    "    \n",
    "    utcTimeMillis         : UTC epoch (1970/1/1)\n",
    "    millisSinceGpsEpoch   : GPS epoch(1980/1/6 midnight 12:00 UTC)\n",
    "    \n",
    "    Ref: https://www.kaggle.com/c/google-smartphone-decimeter-challenge/discussion/239187\n",
    "    '''\n",
    "    dt_offset = pd.to_datetime('1980-01-06 00:00:00') \n",
    "    dt_offset_in_ms = int(dt_offset.value / 1e6)\n",
    "    df['millisSinceGpsEpoch'] = df['utcTimeMillis'] - dt_offset_in_ms + 18000\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_imu_data(data_dir, dataset_name, cname, pname, bl_df):\n",
    "    '''Prepare IMU Dataset (For Train: IMU+GT+BL; For Test: IMU+BL)\n",
    "    Input：\n",
    "        1. data_dir: data_dir\n",
    "        2. dataset_name: dataset name（'train'/'test'）\n",
    "        3. cname: CollectionName\n",
    "        4. pname: phoneName\n",
    "        5. bl_df: baseline's dataframe\n",
    "    Output：df_all\n",
    "    '''\n",
    "    # load GNSS log\n",
    "    gnss_df = gnss_log_to_dataframes(str(data_dir / dataset_name / cname / pname / f'{pname}_GnssLog.txt'))\n",
    "    print('sub-dataset shape：')\n",
    "    print('Raw:', gnss_df['Raw'].shape)\n",
    "    print('Status:', gnss_df['Status'].shape)\n",
    "    print('UncalAccel:', gnss_df['UncalAccel'].shape)\n",
    "    print('UncalGyro:', gnss_df['UncalGyro'].shape)\n",
    "    print('UncalMag:', gnss_df['UncalMag'].shape)\n",
    "    print('OrientationDeg:', gnss_df['OrientationDeg'].shape)\n",
    "    print('Fix:', gnss_df['Fix'].shape)\n",
    "\n",
    "    # merge sub-datasets\n",
    "    # accel + gyro\n",
    "    imu_df = pd.merge_asof(gnss_df['UncalAccel'].sort_values('utcTimeMillis'),\n",
    "                           gnss_df['UncalGyro'].drop('elapsedRealtimeNanos', axis=1).sort_values('utcTimeMillis'),\n",
    "                           on = 'utcTimeMillis',\n",
    "                           direction='nearest')\n",
    "    # (accel + gyro) + mag\n",
    "    imu_df = pd.merge_asof(imu_df.sort_values('utcTimeMillis'),\n",
    "                           gnss_df['UncalMag'].drop('elapsedRealtimeNanos', axis=1).sort_values('utcTimeMillis'),\n",
    "                           on = 'utcTimeMillis',\n",
    "                           direction='nearest')\n",
    "    # ((accel + gyro) + mag) + OrientationDeg\n",
    "    imu_df = pd.merge_asof(imu_df.sort_values('utcTimeMillis'),\n",
    "                           gnss_df['OrientationDeg'].drop('elapsedRealtimeNanos', axis=1).sort_values('utcTimeMillis'),\n",
    "                           on = 'utcTimeMillis',\n",
    "                           direction='nearest')\n",
    "   \n",
    "    # UTC->GpsEpoch\n",
    "    imu_df = UTC2GpsEpoch(imu_df)\n",
    "\n",
    "    # print IMU time\n",
    "    dt_offset = pd.to_datetime('1980-01-06 00:00:00')\n",
    "    dt_offset_in_ms = int(dt_offset.value / 1e6)\n",
    "    tmp_datetime = pd.to_datetime(imu_df['millisSinceGpsEpoch'] + dt_offset_in_ms, unit='ms')\n",
    "    print(f\"imu_df time scope: {tmp_datetime.min()} - {tmp_datetime.max()}\")\n",
    "\n",
    "\n",
    "    if dataset_name == 'train':\n",
    "        # read GT dataset\n",
    "        gt_path = data_dir / dataset_name / cname / pname / 'ground_truth.csv'\n",
    "        gt_df = pd.read_csv(gt_path, usecols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg'])\n",
    "\n",
    "        # print GT time\n",
    "        tmp_datetime = pd.to_datetime(gt_df['millisSinceGpsEpoch'] + dt_offset_in_ms, unit='ms')\n",
    "        print(f\"gt_df time scope: {tmp_datetime.min()} - {tmp_datetime.max()}\")\n",
    "\n",
    "        # merge GT dataset\n",
    "        imu_df = pd.merge_asof(gt_df.sort_values('millisSinceGpsEpoch'),\n",
    "                               imu_df.drop(['elapsedRealtimeNanos'], axis=1).sort_values('millisSinceGpsEpoch'),\n",
    "                               on = 'millisSinceGpsEpoch',\n",
    "                               direction='nearest')\n",
    "    elif dataset_name == 'test':\n",
    "        # merge smaple_df\n",
    "        imu_df = pd.merge_asof(sample_df.sort_values('millisSinceGpsEpoch'),\n",
    "                           imu_df.drop(['elapsedRealtimeNanos'], axis=1).sort_values('millisSinceGpsEpoch'),\n",
    "                           on = 'millisSinceGpsEpoch',\n",
    "                           direction='nearest')\n",
    "\n",
    "    # OrientationDeg -> Rotation Vector\n",
    "    rxs = []\n",
    "    rys = []\n",
    "    rzs = []\n",
    "    for i in range(len(imu_df)):\n",
    "        y_delta = imu_df['rollDeg'].iloc[i]\n",
    "        z_delta = imu_df['yawDeg'].iloc[i]\n",
    "        x_delta = imu_df['pitchDeg'].iloc[i]\n",
    "        rx, ry, rz = an2v(y_delta, z_delta, x_delta)\n",
    "        rxs.append(rx)\n",
    "        rys.append(ry)\n",
    "        rzs.append(rz)\n",
    "\n",
    "    imu_df['ahrsX'] = rxs\n",
    "    imu_df['ahrsY'] = rys\n",
    "    imu_df['ahrsZ'] = rzs\n",
    "\n",
    "    # calibrate sensors' reading\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        imu_df['Accel{}Mps2'.format(axis)] = imu_df['UncalAccel{}Mps2'.format(axis)] - imu_df['Bias{}Mps2'.format(axis)]\n",
    "        imu_df['Gyro{}RadPerSec'.format(axis)] = imu_df['UncalGyro{}RadPerSec'.format(axis)] - imu_df['Drift{}RadPerSec'.format(axis)]\n",
    "        imu_df['Mag{}MicroT'.format(axis)] = imu_df['UncalMag{}MicroT'.format(axis)] - imu_df['Bias{}MicroT'.format(axis)]\n",
    "\n",
    "        # clearn bias features\n",
    "        imu_df.drop(['Bias{}Mps2'.format(axis), 'Drift{}RadPerSec'.format(axis), 'Bias{}MicroT'.format(axis)], axis = 1, inplace = True) \n",
    "\n",
    "    if dataset_name == 'train':\n",
    "        # merge Baseline dataset：imu_df + bl_df = (GT + IMU) + Baseline\n",
    "        df_all = pd.merge(imu_df.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'}),\n",
    "                      bl_df.drop(['phone'], axis=1).rename(columns={'latDeg':'latDeg_bl','lngDeg':'lngDeg_bl'}),\n",
    "                      on = ['collectionName', 'phoneName', 'millisSinceGpsEpoch'])\n",
    "    elif dataset_name == 'test':\n",
    "        df_all = pd.merge(imu_df,\n",
    "              bl_df[(bl_df['collectionName']==cname) & (bl_df['phoneName']==pname)].drop(['phone'], axis=1).rename(columns={'latDeg':'latDeg_bl','lngDeg':'lngDeg_bl'}),\n",
    "              on = ['millisSinceGpsEpoch'])\n",
    "        df_all.drop(['phone'], axis=1, inplace=True)\n",
    "        \n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WGS84_to_ECEF(lat, lon, alt):\n",
    "    # convert to radians\n",
    "    rad_lat = lat * (np.pi / 180.0)\n",
    "    rad_lon = lon * (np.pi / 180.0)\n",
    "    a    = 6378137.0\n",
    "    # f is the flattening factor\n",
    "    finv = 298.257223563\n",
    "    f = 1 / finv   \n",
    "    # e is the eccentricity\n",
    "    e2 = 1 - (1 - f) * (1 - f)    \n",
    "    # N is the radius of curvature in the prime vertical\n",
    "    N = a / np.sqrt(1 - e2 * np.sin(rad_lat) * np.sin(rad_lat))\n",
    "    x = (N + alt) * np.cos(rad_lat) * np.cos(rad_lon)\n",
    "    y = (N + alt) * np.cos(rad_lat) * np.sin(rad_lon)\n",
    "    z = (N * (1 - e2) + alt)        * np.sin(rad_lat)\n",
    "    return x, y, z\n",
    "\n",
    "transformer = pyproj.Transformer.from_crs(\n",
    "    {\"proj\":'geocent', \"ellps\":'WGS84', \"datum\":'WGS84'},\n",
    "    {\"proj\":'latlong', \"ellps\":'WGS84', \"datum\":'WGS84'},)\n",
    "def ECEF_to_WGS84(x,y,z):\n",
    "    lon, lat, alt = transformer.transform(x,y,z,radians=False)\n",
    "    return lon, lat, alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xyz(df_all, dataset_name):\n",
    "    # baseline: lat/lngDeg -> x/y/z\n",
    "    df_all['Xbl'], df_all['Ybl'], df_all['Zbl'] = zip(*df_all.apply(lambda x: WGS84_to_ECEF(x.latDeg_bl, x.lngDeg_bl, x.heightAboveWgs84EllipsoidM), axis=1))\n",
    "    \n",
    "    if dataset_name == 'train':\n",
    "        # gt: lat/lngDeg -> x/y/z\n",
    "        df_all['Xgt'], df_all['Ygt'], df_all['Zgt'] = zip(*df_all.apply(lambda x: WGS84_to_ECEF(x.latDeg_gt, x.lngDeg_gt, x.heightAboveWgs84EllipsoidM), axis=1))\n",
    "        # copy lat/lngDeg\n",
    "        lat_lng_df = df_all[['latDeg_gt','lngDeg_gt', 'latDeg_bl', 'lngDeg_bl']]\n",
    "        df_all.drop(['latDeg_gt','lngDeg_gt', 'latDeg_bl', 'lngDeg_bl'], axis = 1, inplace = True)\n",
    "    elif dataset_name == 'test':\n",
    "        # copy lat/lngDeg\n",
    "        lat_lng_df = df_all[['latDeg_bl', 'lngDeg_bl']]\n",
    "        df_all.drop(['latDeg_bl', 'lngDeg_bl', 'latDeg','lngDeg',], axis = 1, inplace = True)     \n",
    "\n",
    "    return lat_lng_df, df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_train(df_all_train, window_size):\n",
    "    '''prepare training dataset with all aixses'''\n",
    "    tgt_df = df_all_train.copy()\n",
    "    total_len = len(tgt_df) \n",
    "    moving_times = total_len - window_size \n",
    "    \n",
    "    tgt_df.rename(columns = {'yawDeg':'yawZDeg', 'rollDeg':'rollYDeg', 'pitchDeg':'pitchXDeg'}, inplace = True)\n",
    "\n",
    "    feature_cols = [f for f in list(tgt_df) if f not in ['Xgt', 'Ygt', 'Zgt']]\n",
    "\n",
    "    # Historical Feature names\n",
    "    hist_feats = []\n",
    "    for time_flag in range(1, window_size + 1):\n",
    "        for fn in feature_cols:\n",
    "            hist_feats.append(fn + '_' + str(time_flag))\n",
    "\n",
    "    # Window Sliding\n",
    "    # t1 t2 t3 t4 t5 -> t6\n",
    "    # t2 t3 t4 t5 t6 -> t7\n",
    "\n",
    "    # Add historical data \n",
    "    df_train = pd.DataFrame()\n",
    "    features = []\n",
    "    xs = []\n",
    "    ys = []\n",
    "    zs = []\n",
    "\n",
    "    for start_idx in range(moving_times):\n",
    "        feature_list = list()\n",
    "        x_list = list()\n",
    "        y_list = list()\n",
    "        z_list = list()\n",
    "        for window_idx in range(window_size):\n",
    "            feature_list.extend(tgt_df[feature_cols].iloc[start_idx + window_idx,:].to_list())\n",
    "        x_list.append(tgt_df['Xgt'].iloc[start_idx + window_size])\n",
    "        y_list.append(tgt_df['Ygt'].iloc[start_idx + window_size])\n",
    "        z_list.append(tgt_df['Zgt'].iloc[start_idx + window_size])\n",
    "\n",
    "        features.append(feature_list)\n",
    "        xs.extend(x_list)\n",
    "        ys.extend(y_list)\n",
    "        zs.extend(z_list)\n",
    "\n",
    "    df_train = pd.DataFrame(features, columns = hist_feats)\n",
    "    df_train['Xgt'] = xs\n",
    "    df_train['Ygt'] = ys\n",
    "    df_train['Zgt'] = zs\n",
    "    \n",
    "    # clean single-value feature: collectionName_[1-5]\\phoneName_[1-5]\n",
    "    tmp_feats = []\n",
    "    for fn in list(df_train):\n",
    "        if (fn.startswith('collectionName_') == False) and (fn.startswith('phoneName_') == False):\n",
    "            tmp_feats.append(fn)\n",
    "    df_train = df_train[tmp_feats]\n",
    "\n",
    "    # clean time feature\n",
    "    tmp_drop_feats = []\n",
    "    for f in list(df_train):\n",
    "        if (f.startswith('millisSinceGpsEpoch') == True) or (f.startswith('timeSinceFirstFixSeconds') == True) or (f.startswith('utcTimeMillis') == True):\n",
    "            tmp_drop_feats.append(f)\n",
    "    df_train.drop(tmp_drop_feats, axis = 1, inplace = True)\n",
    "    \n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_test(df_all_test, window_size):\n",
    "    '''prepare testing dataset with all aixses'''\n",
    "    tgt_df = df_all_test.copy()\n",
    "    total_len = len(tgt_df) \n",
    "    moving_times = total_len - window_size \n",
    "    \n",
    "    tgt_df.rename(columns = {'yawDeg':'yawZDeg', 'rollDeg':'rollYDeg', 'pitchDeg':'pitchXDeg'}, inplace = True)\n",
    "\n",
    "    feature_cols = [f for f in list(tgt_df) if f not in ['Xgt', 'Ygt', 'Zgt']] \n",
    "    \n",
    "    hist_feats = []\n",
    "    for time_flag in range(1, window_size + 1):\n",
    "        for fn in feature_cols:\n",
    "            hist_feats.append(fn + '_' + str(time_flag))\n",
    "\n",
    "    # t1 t2 t3 t4 t5 -> t6\n",
    "    # t2 t3 t4 t5 t6 -> t7\n",
    "    df_test = pd.DataFrame()\n",
    "    features = []\n",
    "\n",
    "    for start_idx in range(moving_times):\n",
    "        feature_list = list()\n",
    "\n",
    "        for window_idx in range(window_size):\n",
    "            feature_list.extend(tgt_df[feature_cols].iloc[start_idx + window_idx,:].to_list())\n",
    "        features.append(feature_list)\n",
    "\n",
    "    df_test = pd.DataFrame(features, columns = hist_feats)\n",
    "    tmp_feats = []\n",
    "    for fn in list(df_test):\n",
    "        if (fn.startswith('collectionName_') == False) and (fn.startswith('phoneName_') == False):\n",
    "            tmp_feats.append(fn)\n",
    "    df_test = df_test[tmp_feats]\n",
    "\n",
    "    tmp_drop_feats = []\n",
    "    for f in list(df_test):\n",
    "        if (f.startswith('millisSinceGpsEpoch') == True) or (f.startswith('timeSinceFirstFixSeconds') == True) or (f.startswith('utcTimeMillis') == True) or (f.startswith('elapsedRealtimeNanos') == True):\n",
    "            tmp_drop_feats.append(f)\n",
    "    df_test.drop(tmp_drop_feats, axis = 1, inplace = True)\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_other_axis_feats(df_all, tgt_axis):\n",
    "    '''unrelated-aixs features and uncalibrated features'''\n",
    "    # Clean unrelated-aixs features\n",
    "    all_imu_feats = ['UncalAccelXMps2', 'UncalAccelYMps2', 'UncalAccelZMps2',\n",
    "                     'UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec',\n",
    "                     'UncalMagXMicroT', 'UncalMagYMicroT', 'UncalMagZMicroT',\n",
    "                     'ahrsX', 'ahrsY', 'ahrsZ',\n",
    "                     'AccelXMps2', 'AccelYMps2', 'AccelZMps2',\n",
    "                     'GyroXRadPerSec', 'GyroZRadPerSec', 'GyroYRadPerSec',\n",
    "                     'MagXMicroT', 'MagYMicroT', 'MagZMicroT',\n",
    "                     'yawZDeg', 'rollYDeg', 'pitchXDeg',\n",
    "                     'Xbl', 'Ybl', 'Zbl']\n",
    "    tgt_imu_feats = []\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        if axis != tgt_axis:\n",
    "            for f in all_imu_feats:\n",
    "                if f.find(axis) >= 0:\n",
    "                    tgt_imu_feats.append(f)\n",
    "            \n",
    "    tmp_drop_feats = []\n",
    "    for f in list(df_all):\n",
    "        if f.split('_')[0] in tgt_imu_feats:\n",
    "            tmp_drop_feats.append(f)\n",
    "\n",
    "    tgt_df = df_all.drop(tmp_drop_feats, axis = 1)\n",
    "    \n",
    "    # Clean uncalibrated features\n",
    "    uncal_feats = [f for f in list(tgt_df) if f.startswith('Uncal') == True]\n",
    "    tgt_df = tgt_df.drop(uncal_feats, axis = 1)\n",
    "    \n",
    "    return tgt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stat_feats(data, tgt_axis):\n",
    "    for f in ['yawZDeg', 'rollYDeg', 'pitchXDeg']:\n",
    "        if f.find(tgt_axis) >= 0:\n",
    "            ori_feat = f\n",
    "            break\n",
    "            \n",
    "    cont_feats = ['heightAboveWgs84EllipsoidM', 'ahrs{}'.format(tgt_axis),\n",
    "           'Accel{}Mps2'.format(tgt_axis), 'Gyro{}RadPerSec'.format(tgt_axis), 'Mag{}MicroT'.format(tgt_axis),\n",
    "            '{}bl'.format(tgt_axis)] + [ori_feat]\n",
    "    \n",
    "    for f in cont_feats:\n",
    "        data[f + '_' + str(window_size) + '_mean'] = data[[f + f'_{i}' for i in range(1,window_size)]].mean(axis=1)\n",
    "        data[f + '_' + str(window_size) + '_std'] = data[[f + f'_{i}' for i in range(1,window_size)]].std(axis=1)\n",
    "        data[f + '_' + str(window_size) + '_max'] = data[[f + f'_{i}' for i in range(1,window_size)]].max(axis=1)\n",
    "        data[f + '_' + str(window_size) + '_min'] = data[[f + f'_{i}' for i in range(1,window_size)]].min(axis=1)\n",
    "        data[f + '_' + str(window_size) + '_median'] = data[[f + f'_{i}' for i in range(1,window_size)]].median(axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3,Modeling\n",
    "Note: I only use the given axis features for predict the target axis location.  \n",
    "For example, use features contains x-axis to predict the next x location.  \n",
    "More, I used LGBM here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "params = {\n",
    "    'metric':'mse',\n",
    "    'objective':'regression',\n",
    "    'seed':2021,\n",
    "    'boosting_type':'gbdt',\n",
    "    'early_stopping_rounds':10,\n",
    "    'subsample':0.7,\n",
    "    'feature_fraction':0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'reg_lambda': 10\n",
    "}\n",
    "window_size = 30\n",
    "verbose_flag = True\n",
    "folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection name\n",
    "collection_uniq = bl_trn_df['collectionName'].unique()\n",
    "MTV = [i for i in collection_uniq if 'MTV' in i and ('2021-03' in i or '2021-04' in i)]\n",
    "SF = [i for i in collection_uniq if 'SF' in i and ('2021-03' in i or '2021-04' in i)]\n",
    "RWC = [i for i in collection_uniq if 'RWC' in i and ('2021-03' in i or '2021-04' in i)]\n",
    "SVL = [i for i in collection_uniq if 'SVL' in i and ('2021-03' in i or '2021-04' in i)]\n",
    "SJC = [i for i in collection_uniq if 'SJC' in i and ('2021-03' in i or '2021-04' in i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare Training Dataset： 2021-04-22-US-SJC-1_Pixel4\n",
      "Loading ../input/google-smartphone-decimeter-challenge/train/2021-04-22-US-SJC-1/Pixel4/Pixel4_GnssLog.txt\n",
      "sub-dataset shape：\n",
      "Raw: (69759, 36)\n",
      "Status: (105134, 13)\n",
      "UncalAccel: (300038, 8)\n",
      "UncalGyro: (300038, 8)\n",
      "UncalMag: (289948, 8)\n",
      "OrientationDeg: (180408, 5)\n",
      "Fix: (0, 11)\n",
      "imu_df time scope: 2021-04-22 21:02:55.866000 - 2021-04-22 21:51:07.442000\n",
      "gt_df time scope: 2021-04-22 21:02:55.446000 - 2021-04-22 21:51:06.446000\n",
      "____________________\n",
      "Prepare Training Dataset： 2021-04-22-US-SJC-1_SamsungS20Ultra\n",
      "Loading ../input/google-smartphone-decimeter-challenge/train/2021-04-22-US-SJC-1/SamsungS20Ultra/SamsungS20Ultra_GnssLog.txt\n",
      "sub-dataset shape：\n",
      "Raw: (103693, 36)\n",
      "Status: (39416, 13)\n",
      "UncalAccel: (282622, 8)\n",
      "UncalGyro: (282622, 8)\n",
      "UncalMag: (282622, 8)\n",
      "OrientationDeg: (282623, 5)\n",
      "Fix: (0, 11)\n",
      "imu_df time scope: 2021-04-22 21:02:57.127000 - 2021-04-22 21:50:03.236000\n",
      "gt_df time scope: 2021-04-22 21:02:57 - 2021-04-22 21:50:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [03:27<06:54, 207.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________\n",
      "Prepare Training Dataset： 2021-04-28-US-SJC-1_Pixel4\n",
      "Loading ../input/google-smartphone-decimeter-challenge/train/2021-04-28-US-SJC-1/Pixel4/Pixel4_GnssLog.txt\n",
      "sub-dataset shape：\n",
      "Raw: (58202, 36)\n",
      "Status: (82169, 13)\n",
      "UncalAccel: (209435, 8)\n",
      "UncalGyro: (209435, 8)\n",
      "UncalMag: (201994, 8)\n",
      "OrientationDeg: (126160, 5)\n",
      "Fix: (0, 11)\n",
      "imu_df time scope: 2021-04-28 19:59:34.375000 - 2021-04-28 20:33:12.928000\n",
      "gt_df time scope: 2021-04-28 19:59:34.438000 - 2021-04-28 20:33:12.438000\n",
      "____________________\n",
      "Prepare Training Dataset： 2021-04-28-US-SJC-1_SamsungS20Ultra\n",
      "Loading ../input/google-smartphone-decimeter-challenge/train/2021-04-28-US-SJC-1/SamsungS20Ultra/SamsungS20Ultra_GnssLog.txt\n",
      "sub-dataset shape：\n",
      "Raw: (83566, 36)\n",
      "Status: (44529, 13)\n",
      "UncalAccel: (208424, 8)\n",
      "UncalGyro: (208424, 8)\n",
      "UncalMag: (208424, 8)\n",
      "OrientationDeg: (208425, 5)\n",
      "Fix: (0, 11)\n",
      "imu_df time scope: 2021-04-28 19:59:33.164000 - 2021-04-28 20:34:16.989000\n",
      "gt_df time scope: 2021-04-28 19:59:34 - 2021-04-28 20:34:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [05:48<02:48, 168.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________\n",
      "Prepare Training Dataset： 2021-04-29-US-SJC-2_Pixel4\n",
      "Loading ../input/google-smartphone-decimeter-challenge/train/2021-04-29-US-SJC-2/Pixel4/Pixel4_GnssLog.txt\n",
      "sub-dataset shape：\n",
      "Raw: (75693, 36)\n",
      "Status: (114063, 13)\n",
      "UncalAccel: (241840, 8)\n",
      "UncalGyro: (241841, 8)\n",
      "UncalMag: (232787, 8)\n",
      "OrientationDeg: (145695, 5)\n",
      "Fix: (0, 11)\n",
      "imu_df time scope: 2021-04-29 18:59:06.468000 - 2021-04-29 19:37:57.577000\n",
      "gt_df time scope: 2021-04-29 18:59:06.435000 - 2021-04-29 19:37:56.435000\n",
      "____________________\n",
      "Prepare Training Dataset： 2021-04-29-US-SJC-2_SamsungS20Ultra\n",
      "Loading ../input/google-smartphone-decimeter-challenge/train/2021-04-29-US-SJC-2/SamsungS20Ultra/SamsungS20Ultra_GnssLog.txt\n",
      "sub-dataset shape：\n",
      "Raw: (98538, 36)\n",
      "Status: (39835, 13)\n",
      "UncalAccel: (237139, 8)\n",
      "UncalGyro: (237139, 8)\n",
      "UncalMag: (237139, 8)\n",
      "OrientationDeg: (237140, 5)\n",
      "Fix: (0, 11)\n",
      "imu_df time scope: 2021-04-29 18:59:08.892000 - 2021-04-29 19:38:39.990000\n",
      "gt_df time scope: 2021-04-29 18:59:10 - 2021-04-29 19:38:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [08:36<00:00, 172.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________\n",
      "Final Dataset shape： (14333, 843)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: I use SJC's dataset for training \n",
    "# tgt_cns = ['2021-04-22-US-SJC-1', '2021-04-28-US-SJC-1', '2021-04-29-US-SJC-2']\n",
    "tgt_cns = SJC\n",
    "cn2pn_df = bl_trn_df[['collectionName', 'phoneName']].drop_duplicates()\n",
    "\n",
    "df_trains = []\n",
    "lat_lng_df_trains = []\n",
    "for tgt_cn in tqdm(tgt_cns):\n",
    "    pns = list(cn2pn_df[cn2pn_df['collectionName'] == tgt_cn]['phoneName'].values)\n",
    "    for tgt_pn in pns: # collectionに対するphone\n",
    "        print('Prepare Training Dataset：', tgt_cn + '_' + tgt_pn)  \n",
    "        df_all_train = prepare_imu_data(data_dir, 'train', tgt_cn, tgt_pn, bl_trn_df)\n",
    "        lat_lng_df_train, df_all_train = get_xyz(df_all_train, 'train')\n",
    "        df_train = prepare_df_train(df_all_train,  window_size) # 所有轴的数据\n",
    "        df_trains.append(df_train)\n",
    "        lat_lng_df_trains.append(lat_lng_df_train)\n",
    "        print('_'*20)\n",
    "        \n",
    "df_train = pd.concat(df_trains, axis = 0)\n",
    "lat_lng_df_train = pd.concat(lat_lng_df_trains, axis = 0)\n",
    "print('Final Dataset shape：', df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../input/google-smartphone-decimeter-challenge/test/2021-04-29-US-SJC-3/Pixel4/Pixel4_GnssLog.txt\n",
      "sub-dataset shape：\n",
      "Raw: (54937, 36)\n",
      "Status: (83035, 13)\n",
      "UncalAccel: (205625, 8)\n",
      "UncalGyro: (205626, 8)\n",
      "UncalMag: (197761, 8)\n",
      "OrientationDeg: (123877, 5)\n",
      "Fix: (0, 11)\n",
      "imu_df time scope: 2021-04-29 19:52:06.364000 - 2021-04-29 20:25:08.405000\n",
      "df_test: (1949, 840)\n",
      "df_test.columns: Index(['UncalAccelXMps2_1', 'UncalAccelYMps2_1', 'UncalAccelZMps2_1',\n",
      "       'UncalGyroXRadPerSec_1', 'UncalGyroYRadPerSec_1',\n",
      "       'UncalGyroZRadPerSec_1', 'UncalMagXMicroT_1', 'UncalMagYMicroT_1',\n",
      "       'UncalMagZMicroT_1', 'yawZDeg_1',\n",
      "       ...\n",
      "       'AccelYMps2_30', 'GyroYRadPerSec_30', 'MagYMicroT_30', 'AccelZMps2_30',\n",
      "       'GyroZRadPerSec_30', 'MagZMicroT_30', 'heightAboveWgs84EllipsoidM_30',\n",
      "       'Xbl_30', 'Ybl_30', 'Zbl_30'],\n",
      "      dtype='object', length=840)\n"
     ]
    }
   ],
   "source": [
    "# Example: I choose one of SJC collection from the test dataset as my test dataset, you can choose what as you like\n",
    "cname_test = '2021-04-29-US-SJC-3'\n",
    "# pname_test = 'SamsungS20Ultra'\n",
    "pname_test = 'Pixel4'\n",
    "\n",
    "# cname_test = '2021-03-16-US-MTV-2'\n",
    "# pname_test = 'Pixel4Modded'\n",
    "\n",
    "df_all_test = prepare_imu_data(data_dir, 'test', cname_test, pname_test, bl_tst_df)\n",
    "lat_lng_df_test, df_all_test = get_xyz(df_all_test, 'test')\n",
    "df_test = prepare_df_test(df_all_test,  window_size)\n",
    "print('df_test:', df_test.shape)\n",
    "print('df_test.columns:', df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(df_train, df_test, tgt_axis, window_size):\n",
    "    '''For the given axis target to train the model. Also, it has validation and prediciton.'''\n",
    "    df_train = remove_other_axis_feats(df_train, tgt_axis)\n",
    "    df_train = add_stat_feats(df_train, tgt_axis)\n",
    "    df_test = remove_other_axis_feats(df_test, tgt_axis)\n",
    "    df_test = add_stat_feats(df_test, tgt_axis)\n",
    "    \n",
    "    feature_names = [f for f in list(df_train) if f not in ['Xgt', 'Ygt', 'Zgt']]\n",
    "    target = '{}gt'.format(tgt_axis)\n",
    "\n",
    "    kfold = KFold(n_splits=folds, shuffle=True, random_state=params['seed'])\n",
    "\n",
    "    pred_valid = np.zeros((len(df_train),)) \n",
    "    pred_test = np.zeros((len(df_test),)) \n",
    "    scores = []\n",
    "    for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(df_train, df_train[target])):\n",
    "        X_train = df_train.iloc[trn_idx][feature_names]\n",
    "        Y_train = df_train.iloc[trn_idx][target]\n",
    "        X_val = df_train.iloc[val_idx][feature_names]\n",
    "        Y_val = df_train.iloc[val_idx][target]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        lgb_model = model.fit(X_train, \n",
    "                              Y_train,\n",
    "                              eval_names=['train', 'valid'],\n",
    "                              eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                              verbose=0,\n",
    "                              eval_metric=params['metric'],\n",
    "                              early_stopping_rounds=params['early_stopping_rounds'])\n",
    "        pred_valid[val_idx] = lgb_model.predict(X_val, num_iteration =  lgb_model.best_iteration_)\n",
    "        pred_test += lgb_model.predict(df_test[feature_names], num_iteration =  lgb_model.best_iteration_)\n",
    "\n",
    "        scores.append(lgb_model.best_score_['valid']['l2'])\n",
    "    \n",
    "    pred_test = pred_test /  kfold.n_splits\n",
    "    \n",
    "    if verbose_flag == True:\n",
    "        print(\"Each Fold's MSE：{}, Average MSE：{:.4f}\".format([np.round(v,2) for v in scores], np.mean(scores)))\n",
    "        print(\"-\"*60)\n",
    "    return df_train, df_test, pred_valid, pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shugo/Desktop/Competitions/kaggle/outdoor/venv_outdoor/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "Each Fold's MSE：[90.15, 119.88, 188.38, 100.33, 93.24], Average MSE：118.3971\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shugo/Desktop/Competitions/kaggle/outdoor/venv_outdoor/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "Each Fold's MSE：[193.1, 202.32, 374.75, 179.97, 149.17], Average MSE：219.8594\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shugo/Desktop/Competitions/kaggle/outdoor/venv_outdoor/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=0.7 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "Each Fold's MSE：[146.97, 162.02, 422.7, 158.9, 124.52], Average MSE：203.0214\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_train_x, df_test_x, pred_valid_x, pred_test_x = training(df_train, df_test, 'X', window_size)\n",
    "df_train_y, df_test_y, pred_valid_y, pred_test_y = training(df_train, df_test, 'Y', window_size)\n",
    "df_train_z, df_test_z, pred_valid_z, pred_test_z = training(df_train, df_test, 'Z', window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_compare_df = pd.DataFrame({'Xgt':df_train_x['Xgt'].values, 'Xpred':pred_valid_x,\n",
    "                               'Ygt':df_train_y['Ygt'].values, 'Ypred':pred_valid_y,\n",
    "                                'Zgt':df_train_z['Zgt'].values, 'Zpred':pred_valid_z})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xyz -> lng, lat\n",
    "lng_gt, lat_gt, _ = ECEF_to_WGS84(val_compare_df['Xgt'].values,val_compare_df['Ygt'].values,val_compare_df['Zgt'].values)\n",
    "lng_pred, lat_pred, _ = ECEF_to_WGS84(val_compare_df['Xpred'].values,val_compare_df['Ypred'].values,val_compare_df['Zpred'].values)\n",
    "lng_test_pred, lat_test_pred, _ = ECEF_to_WGS84(pred_test_x, pred_test_y, pred_test_z)\n",
    "\n",
    "\n",
    "val_compare_df['latDeg_gt'] = lat_gt\n",
    "val_compare_df['lngDeg_gt'] = lng_gt\n",
    "val_compare_df['latDeg_pred'] = lat_pred\n",
    "val_compare_df['lngDeg_pred'] = lng_pred\n",
    "test_pred_df = pd.DataFrame({'latDeg':lat_test_pred, 'lngDeg':lng_test_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From：https://www.kaggle.com/emaerthin/demonstration-of-the-kalman-filter\n",
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the great circle distance between two points\n",
    "    on the earth. Inputs are array-like and specified in decimal degrees.\n",
    "    \"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(a**0.5)\n",
    "    dist = 6_367_000 * c\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shugo/Desktop/Competitions/kaggle/outdoor/venv_outdoor/lib/python3.7/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "# インデックス累積和\n",
    "N = [0] + [len(bl_trn_df[bl_trn_df['collectionName']==SJC[n]]) for n in range(len(SJC))]\n",
    "N = [i for i in itertools.accumulate(N)]\n",
    "\n",
    "# trainの結果を反映させる\n",
    "for i in range(len(N)-1):\n",
    "    bl_trn_df[bl_trn_df['collectionName']==SJC[i]].loc[:,'latDeg'] = val_compare_df['latDeg_pred'][N[i]:N[i+1]]\n",
    "    bl_trn_df[bl_trn_df['collectionName']==SJC[i]].loc[:,'lngDeg'] = val_compare_df['lngDeg_pred'][N[i]:N[i+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist_50: 6.523611331179955\n",
      "dist_95: 38.87085992092085\n",
      "avg_dist_50_95: 22.697235626050404\n",
      "avg_dist: 11.756682178606797\n"
     ]
    }
   ],
   "source": [
    "# Baseline vs. GT\n",
    "lat_lng_df_train['dist'] = calc_haversine(\n",
    "                                    lat_lng_df_train.latDeg_gt.reset_index(drop=True), \n",
    "                                    lat_lng_df_train.lngDeg_gt.reset_index(drop=True),\n",
    "                                    bl_trn_df[bl_trn_df['collectionName'].isin(SJC)]['latDeg'].reset_index(drop=True),\n",
    "                                    bl_trn_df[bl_trn_df['collectionName'].isin(SJC)]['lngDeg'].reset_index(drop=True))\n",
    "print('dist_50:',np.percentile(lat_lng_df_train['dist'],50) )\n",
    "print('dist_95:',np.percentile(lat_lng_df_train['dist'],95) )\n",
    "print('avg_dist_50_95:',(np.percentile(lat_lng_df_train['dist'],50) + np.percentile(lat_lng_df_train['dist'],95))/2)\n",
    "print('avg_dist:', lat_lng_df_train['dist'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist_50: 6.129357526334204\n",
      "dist_95: 33.837015811852936\n",
      "avg_dist_50_95: 19.98318666909357\n",
      "avg_dist: 10.508449405316306\n"
     ]
    }
   ],
   "source": [
    "# Baseline vs. GT\n",
    "lat_lng_df_train['dist'] = calc_haversine(\n",
    "lat_lng_df_train.latDeg_gt, lat_lng_df_train.lngDeg_gt,                                      lat_lng_df_train.latDeg_bl, lat_lng_df_train.lngDeg_bl)\n",
    "\n",
    "print('dist_50:',np.percentile(lat_lng_df_train['dist'],50) )\n",
    "print('dist_95:',np.percentile(lat_lng_df_train['dist'],95) )\n",
    "print('avg_dist_50_95:',(np.percentile(lat_lng_df_train['dist'],50) + np.percentile(lat_lng_df_train['dist'],95))/2)\n",
    "print('avg_dist:', lat_lng_df_train['dist'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist_50: 6.839893596084978\n",
      "dist_95: 19.370614100148384\n",
      "avg_dist_50_95: 13.10525384811668\n",
      "avg_dist: 8.366828373091364\n"
     ]
    }
   ],
   "source": [
    "# IMU Prediction vs. GT\n",
    "val_compare_df['dist'] = calc_haversine(\n",
    "val_compare_df.latDeg_gt,                                        val_compare_df.lngDeg_gt, \n",
    "val_compare_df.latDeg_pred,                                     val_compare_df.lngDeg_pred)\n",
    "# IMU预测vsGT（多collection）\n",
    "print('dist_50:',np.percentile(val_compare_df['dist'],50) )\n",
    "print('dist_95:',np.percentile(val_compare_df['dist'],95) )\n",
    "print('avg_dist_50_95:',(np.percentile(val_compare_df['dist'],50) + np.percentile(val_compare_df['dist'],95))/2)\n",
    "print('avg_dist:', val_compare_df['dist'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_tst_df.iloc[bl_tst_df[bl_tst_df['phone']==cname_test + '_' + pname_test].index[window_size:],3] = test_pred_df['latDeg'].values\n",
    "bl_tst_df.iloc[bl_tst_df[bl_tst_df['phone']==cname_test + '_' + pname_test].index[window_size:],4] = test_pred_df['lngDeg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_trn_df.to_csv('../output/filtered_nb037.csv', index=False)\n",
    "bl_tst_df.to_csv('../output/sub_nb037.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('venv_outdoor': venv)",
   "metadata": {
    "interpreter": {
     "hash": "bd00b75c79969edcf008edd1fd5973862c0c93beffacd004fb7d75ad6fcb357f"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}